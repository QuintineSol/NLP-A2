{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic\n",
        "!pip install FuzzyTM\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade umap-learn\n",
        "!pip install --upgrade bertopic\n",
        "!pip install --upgrade gensim\n",
        "!pip install --upgrade spacy\n",
        "!pip install --update numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vzm1RLpKE4aM",
        "outputId": "82c7944f-ca9c-459c-daf2-3a9e7c9cbda7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.39)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.5)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Collecting numpy>=1.20.0 (from bertopic)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Requirement already satisfied: FuzzyTM in /usr/local/lib/python3.10/dist-packages (2.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from FuzzyTM) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from FuzzyTM) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from FuzzyTM) (1.10.1)\n",
            "Requirement already satisfied: pyfume in /usr/local/lib/python3.10/dist-packages (from FuzzyTM) (0.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->FuzzyTM) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->FuzzyTM) (2024.2)\n",
            "Collecting numpy (from FuzzyTM)\n",
            "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: simpful==2.12.0 in /usr/local/lib/python3.10/dist-packages (from pyfume->FuzzyTM) (2.12.0)\n",
            "Requirement already satisfied: fst-pso==1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyfume->FuzzyTM) (1.8.1)\n",
            "Requirement already satisfied: miniful in /usr/local/lib/python3.10/dist-packages (from fst-pso==1.8.1->pyfume->FuzzyTM) (0.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM) (1.16.0)\n",
            "Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "8b9b61cf2c8d4cb095a72ee39b718418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.24.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.39)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.24.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.5)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.24.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.3.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.24.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --update\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP - Assignment 2"
      ],
      "metadata": {
        "id": "9rYAx4e2IoF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "h6Me-aAIIucC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from bertopic import BERTopic\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from FuzzyTM import FLSA_W\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "eJpG9EHvI1ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "KJrknAepI-ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./us_equities_news_dataset.csv\"\n",
        "\n",
        "# Load the news dataset\n",
        "news_dataset = pd.read_csv(\"./us_equities_news_dataset.csv\")\n",
        "news_dataset.head()"
      ],
      "metadata": {
        "id": "NLJ5KEEQJB1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset to only include articles with 'Nvidia' in the content\n",
        "nvidia_dataset = news_dataset[news_dataset['content'].str.contains('Nvidia', case=False, na=False)]"
      ],
      "metadata": {
        "id": "hlbTI6GjJE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate title and content columns\n",
        "nvidia_dataset['content'] = nvidia_dataset['title'] + ' ' + nvidia_dataset['content']"
      ],
      "metadata": {
        "id": "WpjzmJNYJXXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text,\n",
        "                    remove_punctuation=True,\n",
        "                    remove_stopwords=True,\n",
        "                    lemmatize=False,\n",
        "                    stem=False,\n",
        "                    remove_short_words=False,\n",
        "                    remove_rare_words=False,\n",
        "                    remove_numbers=True,\n",
        "                    min_word_length=2):\n",
        "    \"\"\"\n",
        "    Advanced preprocessing function that applies different levels of text processing.\n",
        "\n",
        "    Parameters:\n",
        "    - text: The text to preprocess.\n",
        "    - remove_punctuation: Whether to remove punctuation from the text.\n",
        "    - remove_stopwords: Whether to remove common stopwords.\n",
        "    - lemmatize: Whether to apply lemmatization to reduce words to their root form.\n",
        "    - stem: Whether to apply stemming to reduce words to their base form.\n",
        "    - remove_short_words: Whether to remove short words from the text.\n",
        "    - remove_rare_words: Whether to remove rare words based on the dataset distribution.\n",
        "    - remove_numbers: Whether to remove numbers from the text.\n",
        "    - min_word_length: The minimum length of words to keep in the text.\n",
        "\n",
        "    Returns:\n",
        "    - Preprocessed text as tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove numbers if specified\n",
        "    if remove_numbers:\n",
        "        tokens = [re.sub(r'\\d+', '', token) for token in tokens]\n",
        "\n",
        "    # Remove non-alphabetic characters (punctuation)\n",
        "    if remove_punctuation:\n",
        "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    if lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Stemming (alternative to lemmatization)\n",
        "    if stem:\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Remove short words\n",
        "    if remove_short_words:\n",
        "        tokens = [token for token in tokens if len(token) >= min_word_length]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Rj42yjhDJNeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_preprocessing(nvidia_dataset, version='v1'):\n",
        "    \"\"\"\n",
        "    Apply different levels of preprocessing to the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - nvidia_dataset: The Nvidia articles with a 'content' column.\n",
        "    - version: The version of preprocessing to apply ('v1', 'v2', 'v3', or 'v4').\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with the original content and the preprocessed content in 'preprocessed_content' column.\n",
        "    \"\"\"\n",
        "\n",
        "    if version == 'v1':\n",
        "        # Basic tokenization and lowercasing\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=False, remove_stopwords=False,\n",
        "            lemmatize=False, remove_numbers=False, stem=False,\n",
        "            remove_short_words=False\n",
        "        ))\n",
        "\n",
        "    elif version == 'v2':\n",
        "        # Remove punctuation, stopwords, and numbers, but no lemmatization/stemming\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=False, remove_numbers=False, stem=False,\n",
        "            remove_short_words=False\n",
        "        ))\n",
        "\n",
        "    elif version == 'v3':\n",
        "        # Advanced preprocessing with stemming, number removal, short words removal\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=False, stem=True, remove_numbers=True,\n",
        "            remove_short_words=True, min_word_length=2\n",
        "        ))\n",
        "\n",
        "    elif version == 'v4':\n",
        "        # Full preprocessing with lemmatization instead of stemming, number removal, and short words removal\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=True, remove_numbers=True, stem=False,\n",
        "            remove_short_words=True, min_word_length=2\n",
        "        ))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid preprocessing version. Choose from 'v1', 'v2', 'v3', or 'v4'.\")\n",
        "\n",
        "    # Return the DataFrame with original and preprocessed content\n",
        "    return nvidia_dataset"
      ],
      "metadata": {
        "id": "kwM0BdG7JbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_v1 = apply_preprocessing(nvidia_dataset, version='v1')  # Basic preprocessing\n",
        "train_data_v2 = apply_preprocessing(nvidia_dataset, version='v2')  # Intermediate preprocessing\n",
        "train_data_v3 = apply_preprocessing(nvidia_dataset, version='v3')  # Full preprocessing with stemming\n",
        "train_data_v4 = apply_preprocessing(nvidia_dataset, version='v4')  # Full preprocessing with lemmatizer\n",
        "\n",
        "train_data_v1.head()"
      ],
      "metadata": {
        "id": "MbJ5SDCFJfX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data_head = train_data_v1.head()\n",
        "\n",
        "# # Specify the filename for the Excel file\n",
        "# output_file = 'train_data_v1_head.xlsx'\n",
        "\n",
        "# # Save to Excel\n",
        "# train_data_head.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "9molEoeAJhal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "IzHaJpSUJno8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "3fqtU4TAwSlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values_lda(train_data_list, dictionary, max_topics=20):\n",
        "    \"\"\"\n",
        "    Compute coherence values for different numbers of topics in LDA model.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data_list: List of tokenized documents.\n",
        "    - dictionary: Gensim dictionary.\n",
        "    - max_topics: Maximum number of topics to test.\n",
        "\n",
        "    Returns:\n",
        "    - coherence_values: List of coherence values for each number of topics.\n",
        "    - optimal_num_topics: The optimal number of topics based on maximum coherence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document-Term Matrix\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_data_list]\n",
        "\n",
        "    coherence_values = []\n",
        "\n",
        "    # Iterate over different numbers of topics\n",
        "    for num_topics in range(2, max_topics + 1):\n",
        "        lda_model = gensim.models.LdaModel(\n",
        "            doc_term_matrix,\n",
        "            num_topics=num_topics,\n",
        "            id2word=dictionary,\n",
        "            random_state=42,\n",
        "            passes=10,\n",
        "            alpha='auto'\n",
        "        )\n",
        "\n",
        "        # Compute coherence score\n",
        "        coherence_model_lda = CoherenceModel(model=lda_model, texts=train_data_list, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherence_model_lda.get_coherence())\n",
        "\n",
        "    # Find the optimal number of topics (maximum coherence)\n",
        "    optimal_num_topics = coherence_values.index(max(coherence_values)) + 2  # +2 because range starts from 2\n",
        "\n",
        "    return coherence_values, optimal_num_topics"
      ],
      "metadata": {
        "id": "SZY9YggPJyr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_coherence_scores_lda(coherence_values, max_topics):\n",
        "    \"\"\"\n",
        "    Plot the coherence scores to visualize the elbow method.\n",
        "\n",
        "    Parameters:\n",
        "    - coherence_values: List of coherence values.\n",
        "    - max_topics: Maximum number of topics tested.\n",
        "    \"\"\"\n",
        "    x = range(2, max_topics + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, coherence_values, marker='o')\n",
        "    plt.title('Coherence Scores vs. Number of Topics')\n",
        "    plt.xlabel('Number of Topics')\n",
        "    plt.ylabel('Coherence Score')\n",
        "    plt.xticks(x)\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WPdyGgy3J1Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "GdYmOMGOwzj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_bertopic_topics(topic_model):\n",
        "    \"\"\"\n",
        "    Print topics generated by BERTopic.\n",
        "\n",
        "    Parameters:\n",
        "    - topic_model: the trained BERTopic model\n",
        "    \"\"\"\n",
        "    topics = topic_model.get_topics()\n",
        "    for topic_num, words in topics.items():\n",
        "        # Ignore the '-1' topic, which is typically noise in BERTopic\n",
        "        if topic_num == -1:\n",
        "            continue\n",
        "        print(f\"Topic {topic_num}: {', '.join([word[0] for word in words])}\")"
      ],
      "metadata": {
        "id": "CTcmazRCKGWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Topic Models"
      ],
      "metadata": {
        "id": "3GDtC3K6J2-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_topic_model(train_data, model_type='LDA', max_topics=20):\n",
        "    \"\"\"\n",
        "    Train a topic model on the given training data.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data: list of str, the text to train the model on\n",
        "    - model_type: str, the type of model to train ('LDA', 'FLSA-W', 'BERTopic')\n",
        "    - num_topics: int, the number of topics to generate\n",
        "\n",
        "    Returns:\n",
        "    - model: the trained model\n",
        "    - topics: the topics generated by the model\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_list_string = [' '.join(tokens) for tokens in train_data['preprocessed_content']]\n",
        "    train_data_list_tokens = train_data['preprocessed_content'].tolist()  # This should already be a list of lists\n",
        "\n",
        "\n",
        "    if model_type == 'LDA':\n",
        "        # Create Gensim dictionary\n",
        "        dictionary = corpora.Dictionary(train_data_list_tokens)\n",
        "\n",
        "        # Compute coherence values and find optimal number of topics\n",
        "        coherence_values, optimal_num_topics = compute_coherence_values_lda(train_data_list_tokens, dictionary, max_topics)\n",
        "\n",
        "        # Plot coherence scores\n",
        "        plot_coherence_scores_lda(coherence_values, max_topics)\n",
        "\n",
        "        # Create Document-Term Matrix\n",
        "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_data_list_tokens]\n",
        "\n",
        "        # Train LDA model with optimal number of topics\n",
        "        lda_model = gensim.models.LdaModel(\n",
        "            doc_term_matrix,\n",
        "            num_topics=optimal_num_topics,\n",
        "            id2word=dictionary,\n",
        "            random_state=42,\n",
        "            passes=10,\n",
        "            alpha='auto'\n",
        "        )\n",
        "\n",
        "        # Get topics (top words in each topic)\n",
        "        topics = lda_model.print_topics(num_words=10)\n",
        "\n",
        "        return lda_model, topics\n",
        "\n",
        "    elif model_type == 'FLSA-W':\n",
        "        # Tokenized input is passed directly to FLSA-W\n",
        "        tokenized_data = [tokens for tokens in train_data['preprocessed_content']]\n",
        "\n",
        "        # Initialize FLSA-W model with tokenized data directly\n",
        "        flsa_w_model = FLSA_W(\n",
        "            input_file=tokenized_data,\n",
        "            num_topics=10,\n",
        "            num_words=10\n",
        "        )\n",
        "\n",
        "        # Train the FLSA-W model\n",
        "        pwgt, ptgd = flsa_w_model.get_matrices()  # This trains the model\n",
        "\n",
        "        # Get topics as words\n",
        "        topics = flsa_w_model.show_topics(representation='words')\n",
        "\n",
        "        return flsa_w_model, topics\n",
        "\n",
        "    elif model_type == 'BERTopic':\n",
        "        # Train BERTopic model\n",
        "        topic_model = BERTopic()\n",
        "        topics, _ = topic_model.fit_transform(train_data_list_string)\n",
        "\n",
        "        return topic_model, topics\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_type. Choose from 'LDA', 'FLSA-W', 'BERTopic'.\")"
      ],
      "metadata": {
        "id": "-Lk7x-51J8wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_versions(data_versions, model_type, max_topics=20):\n",
        "    \"\"\"\n",
        "    Train a topic model on different versions of the data.\n",
        "\n",
        "    Parameters:\n",
        "    - data_versions: list of DataFrames, each containing a separate version of preprocessed data.\n",
        "    - model_type: str, the type of model to train ('LDA', 'FLSA-W', 'BERTopic')\n",
        "    - max_topics: int, the maximum number of topics to generate\n",
        "\n",
        "    Returns:\n",
        "    - models: list of trained models\n",
        "    - topics: list of topics generated by the models\n",
        "    \"\"\"\n",
        "\n",
        "    models = []\n",
        "    topics = []\n",
        "\n",
        "    # Train model on different versions\n",
        "    for data_v in data_versions:\n",
        "        print(f\"Training model on version: {data_v}\")\n",
        "        model, topic = train_topic_model(data_v, model_type=model_type, max_topics=max_topics)\n",
        "        print(f\"Finished training model on version: {data_v}\")\n",
        "        models.append(model)\n",
        "        topics.append(topic)\n",
        "\n",
        "    return models, topics"
      ],
      "metadata": {
        "id": "7b-vEbOayGaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration 1"
      ],
      "metadata": {
        "id": "de0Jh4K3J_o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "sTZJ44yQKIYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LDA_model, LDA_topics = train_topic_model(train_data_v1, model_type='LDA', max_topics=20)\n",
        "\n",
        "# Print topics\n",
        "for topic in LDA_topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "810-_HuwNVTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FLSA-W"
      ],
      "metadata": {
        "id": "Nm02RABvNXYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FLSA-W model on the first version of the preprocessed dataset\n",
        "flsa_w_model, flsa_w_topics = train_topic_model(train_data_v1, model_type='FLSA-W', max_topics=10)\n",
        "\n",
        "# Print topics\n",
        "for topic in flsa_w_topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "zz1RW5f9Ni9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print the generated FLSA-W topics\n",
        "# print(\"Initial FLSA-W Topics:\")\n",
        "# print_topics_lda(flsa_w_topics)"
      ],
      "metadata": {
        "id": "QIkFdv0ZNli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_flsa_w(train_data, max_topics=10):\n",
        "    \"\"\"\n",
        "    Compute coherence scores for FLSA-W model with varying number of topics.\n",
        "    Parameters:\n",
        "        - train_data: list of tokenized documents (preprocessed content).\n",
        "        - max_topics: int, the maximum number of topics to test.\n",
        "    Returns:\n",
        "        - topic_nums: list of topic numbers used for testing.\n",
        "        - coherence_scores: list of coherence scores for each number of topics.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_data = [tokens for tokens in train_data['preprocessed_content']]\n",
        "\n",
        "    topic_nums = []\n",
        "    coherence_scores = []\n",
        "\n",
        "    # Try different numbers of topics\n",
        "    for num_topics in range(3, max_topics + 1):\n",
        "        # Train FLSA-W model\n",
        "        flsa_w_model = FLSA_W(\n",
        "            input_file=tokenized_data,\n",
        "            num_topics=num_topics,\n",
        "            num_words=10\n",
        "        )\n",
        "\n",
        "        # Train the FLSA-W model\n",
        "        flsa_w_model.get_matrices()\n",
        "\n",
        "        # Get the coherence score for the current model\n",
        "        coherence_score = flsa_w_model.get_coherence_score()\n",
        "        print(f\"Number of topics: {num_topics}, Coherence score: {coherence_score}\")\n",
        "\n",
        "        # Store the results\n",
        "        topic_nums.append(num_topics)\n",
        "        coherence_scores.append(coherence_score)\n",
        "\n",
        "    return topic_nums, coherence_scores\n",
        "\n",
        "\n",
        "def plot_elbow_curve(topic_nums, coherence_scores):\n",
        "    \"\"\"\n",
        "    Create an elbow plot for the coherence scores vs. number of topics.\n",
        "\n",
        "    Parameters:\n",
        "    - topic_nums: list of number of topics tested.\n",
        "    - coherence_scores: list of coherence scores for each number of topics.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(topic_nums, coherence_scores, marker='o')\n",
        "    plt.title('FLSA-W Elbow Plot: Number of Topics vs Coherence Score')\n",
        "    plt.xlabel('Number of Topics')\n",
        "    plt.ylabel('Coherence Score')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F-nTTmxIN7jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and compute coherence scores\n",
        "topic_nums, coherence_scores = compute_coherence_flsa_w(train_data_v1, max_topics=15)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plot_elbow_curve(topic_nums, coherence_scores)"
      ],
      "metadata": {
        "id": "7EhS5j6fOLQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "j8JwRdpeONqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v1, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ],
      "metadata": {
        "id": "KDhIsPONOXdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "NuFOEwWUz1yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration 2"
      ],
      "metadata": {
        "id": "bYY_NWeBOam7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v2, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ],
      "metadata": {
        "id": "3rk6CVxSz1yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration 3"
      ],
      "metadata": {
        "id": "sCrN0l0oOcXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "0Hxrgkgcz5QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v3, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ],
      "metadata": {
        "id": "FL6HDDnMz5QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration 4"
      ],
      "metadata": {
        "id": "IFUa24MPOeKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "Hhkqta5Fz6mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v4, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ],
      "metadata": {
        "id": "oTDDmxTVz6mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Similarity Matrix"
      ],
      "metadata": {
        "id": "4fArTnJ50J7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertopic_model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "SmkYFtmt0GXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "iuoBS0hPOgJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative"
      ],
      "metadata": {
        "id": "E9cBzPruOiXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative"
      ],
      "metadata": {
        "id": "OkdD5fS0OmLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output of Final Topic Model"
      ],
      "metadata": {
        "id": "nxD_9xRpOn4c"
      }
    }
  ]
}
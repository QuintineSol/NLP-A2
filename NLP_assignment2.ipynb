{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rYAx4e2IoF2"
      },
      "source": [
        "# NLP - Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Me-aAIIucC"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJpG9EHvI1ni"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from bertopic import BERTopic\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from FuzzyTM import FLSA_W\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim import corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJrknAepI-ey"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLJ5KEEQJB1d"
      },
      "outputs": [],
      "source": [
        "path = \"./us_equities_news_dataset.csv\"\n",
        "\n",
        "# Load the news dataset\n",
        "news_dataset = pd.read_csv(\"./us_equities_news_dataset.csv\")\n",
        "news_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlbTI6GjJE-m"
      },
      "outputs": [],
      "source": [
        "# Filter dataset to only include articles with 'Nvidia' in the content\n",
        "nvidia_dataset = news_dataset[news_dataset['content'].str.contains('Nvidia', case=False, na=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpjzmJNYJXXB"
      },
      "outputs": [],
      "source": [
        "# Concatenate title and content columns\n",
        "nvidia_dataset['content'] = nvidia_dataset['title'] + ' ' + nvidia_dataset['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj42yjhDJNeq"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text,\n",
        "                    remove_punctuation=True,\n",
        "                    remove_stopwords=True,\n",
        "                    lemmatize=False,\n",
        "                    stem=False,\n",
        "                    remove_short_words=False,\n",
        "                    remove_rare_words=False,\n",
        "                    remove_numbers=True,\n",
        "                    min_word_length=2):\n",
        "    \"\"\"\n",
        "    Advanced preprocessing function that applies different levels of text processing.\n",
        "\n",
        "    Parameters:\n",
        "    - text: The text to preprocess.\n",
        "    - remove_punctuation: Whether to remove punctuation from the text.\n",
        "    - remove_stopwords: Whether to remove common stopwords.\n",
        "    - lemmatize: Whether to apply lemmatization to reduce words to their root form.\n",
        "    - stem: Whether to apply stemming to reduce words to their base form.\n",
        "    - remove_short_words: Whether to remove short words from the text.\n",
        "    - remove_rare_words: Whether to remove rare words based on the dataset distribution.\n",
        "    - remove_numbers: Whether to remove numbers from the text.\n",
        "    - min_word_length: The minimum length of words to keep in the text.\n",
        "\n",
        "    Returns:\n",
        "    - Preprocessed text as tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove numbers if specified\n",
        "    if remove_numbers:\n",
        "        tokens = [re.sub(r'\\d+', '', token) for token in tokens]\n",
        "\n",
        "    # Remove non-alphabetic characters (punctuation)\n",
        "    if remove_punctuation:\n",
        "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    if lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Stemming (alternative to lemmatization)\n",
        "    if stem:\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Remove short words\n",
        "    if remove_short_words:\n",
        "        tokens = [token for token in tokens if len(token) >= min_word_length]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwM0BdG7JbVQ"
      },
      "outputs": [],
      "source": [
        "def apply_preprocessing(nvidia_dataset, version='v1'):\n",
        "    \"\"\"\n",
        "    Apply different levels of preprocessing to the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - nvidia_dataset: The Nvidia articles with a 'content' column.\n",
        "    - version: The version of preprocessing to apply ('v1', 'v2', 'v3', or 'v4').\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with the original content and the preprocessed content in 'preprocessed_content' column.\n",
        "    \"\"\"\n",
        "\n",
        "    if version == 'v1':\n",
        "        # Basic tokenization and lowercasing\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=False, remove_stopwords=False,\n",
        "            lemmatize=False, remove_numbers=False, stem=False,\n",
        "            remove_short_words=False\n",
        "        ))\n",
        "\n",
        "    elif version == 'v2':\n",
        "        # Remove punctuation, stopwords, and numbers, but no lemmatization/stemming\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=False, remove_numbers=False, stem=False,\n",
        "            remove_short_words=False\n",
        "        ))\n",
        "\n",
        "    elif version == 'v3':\n",
        "        # Advanced preprocessing with stemming, number removal, short words removal\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=False, stem=True, remove_numbers=True,\n",
        "            remove_short_words=True, min_word_length=2\n",
        "        ))\n",
        "\n",
        "    elif version == 'v4':\n",
        "        # Full preprocessing with lemmatization instead of stemming, number removal, and short words removal\n",
        "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
        "            x, remove_punctuation=True, remove_stopwords=True,\n",
        "            lemmatize=True, remove_numbers=True, stem=False,\n",
        "            remove_short_words=True, min_word_length=2\n",
        "        ))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid preprocessing version. Choose from 'v1', 'v2', 'v3', or 'v4'.\")\n",
        "\n",
        "    # Return the DataFrame with original and preprocessed content\n",
        "    return nvidia_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbJ5SDCFJfX3"
      },
      "outputs": [],
      "source": [
        "train_data_v1 = apply_preprocessing(nvidia_dataset, version='v1')  # Basic preprocessing\n",
        "train_data_v2 = apply_preprocessing(nvidia_dataset, version='v2')  # Intermediate preprocessing\n",
        "train_data_v3 = apply_preprocessing(nvidia_dataset, version='v3')  # Full preprocessing with stemming\n",
        "train_data_v4 = apply_preprocessing(nvidia_dataset, version='v4')  # Full preprocessing with lemmatizer\n",
        "\n",
        "train_data_v1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9molEoeAJhal"
      },
      "outputs": [],
      "source": [
        "# train_data_head = train_data_v1.head()\n",
        "\n",
        "# # Specify the filename for the Excel file\n",
        "# output_file = 'train_data_v1_head.xlsx'\n",
        "\n",
        "# # Save to Excel\n",
        "# train_data_head.to_excel(output_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzHaJpSUJno8"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fqtU4TAwSlt"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZY9YggPJyr4"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values_lda(train_data_list, dictionary, max_topics=20):\n",
        "    \"\"\"\n",
        "    Compute coherence values for different numbers of topics in LDA model.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data_list: List of tokenized documents.\n",
        "    - dictionary: Gensim dictionary.\n",
        "    - max_topics: Maximum number of topics to test.\n",
        "\n",
        "    Returns:\n",
        "    - coherence_values: List of coherence values for each number of topics.\n",
        "    - optimal_num_topics: The optimal number of topics based on maximum coherence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Document-Term Matrix\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_data_list]\n",
        "\n",
        "    coherence_values = []\n",
        "\n",
        "    # Iterate over different numbers of topics\n",
        "    for num_topics in range(2, max_topics + 1):\n",
        "        lda_model = gensim.models.LdaModel(\n",
        "            doc_term_matrix,\n",
        "            num_topics=num_topics,\n",
        "            id2word=dictionary,\n",
        "            random_state=42,\n",
        "            passes=10,\n",
        "            alpha='auto'\n",
        "        )\n",
        "\n",
        "        # Compute coherence score\n",
        "        coherence_model_lda = CoherenceModel(model=lda_model, texts=train_data_list, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherence_model_lda.get_coherence())\n",
        "\n",
        "    # Find the optimal number of topics (maximum coherence)\n",
        "    optimal_num_topics = coherence_values.index(max(coherence_values)) + 2  # +2 because range starts from 2\n",
        "\n",
        "    return coherence_values, optimal_num_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPdyGgy3J1Kz"
      },
      "outputs": [],
      "source": [
        "def plot_coherence_scores_lda(coherence_values, max_topics):\n",
        "    \"\"\"\n",
        "    Plot the coherence scores to visualize the elbow method.\n",
        "\n",
        "    Parameters:\n",
        "    - coherence_values: List of coherence values.\n",
        "    - max_topics: Maximum number of topics tested.\n",
        "    \"\"\"\n",
        "    x = range(2, max_topics + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, coherence_values, marker='o')\n",
        "    plt.title('Coherence Scores vs. Number of Topics')\n",
        "    plt.xlabel('Number of Topics')\n",
        "    plt.ylabel('Coherence Score')\n",
        "    plt.xticks(x)\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdYmOMGOwzj5"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTcmazRCKGWh"
      },
      "outputs": [],
      "source": [
        "def print_bertopic_topics(topic_model):\n",
        "    \"\"\"\n",
        "    Print topics generated by BERTopic.\n",
        "\n",
        "    Parameters:\n",
        "    - topic_model: the trained BERTopic model\n",
        "    \"\"\"\n",
        "    topics = topic_model.get_topics()\n",
        "    for topic_num, words in topics.items():\n",
        "        # Ignore the '-1' topic, which is typically noise in BERTopic\n",
        "        if topic_num == -1:\n",
        "            continue\n",
        "        print(f\"Topic {topic_num}: {', '.join([word[0] for word in words])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GDtC3K6J2-7"
      },
      "source": [
        "## Train Topic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lk7x-51J8wR"
      },
      "outputs": [],
      "source": [
        "def train_topic_model(train_data, model_type='LDA', max_topics=20):\n",
        "    \"\"\"\n",
        "    Train a topic model on the given training data.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data: list of str, the text to train the model on\n",
        "    - model_type: str, the type of model to train ('LDA', 'FLSA-W', 'BERTopic')\n",
        "    - num_topics: int, the number of topics to generate\n",
        "\n",
        "    Returns:\n",
        "    - model: the trained model\n",
        "    - topics: the topics generated by the model\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_list_string = [' '.join(tokens) for tokens in train_data['preprocessed_content']]\n",
        "    train_data_list_tokens = train_data['preprocessed_content'].tolist()  # This should already be a list of lists\n",
        "\n",
        "\n",
        "    if model_type == 'LDA':\n",
        "        # Create Gensim dictionary\n",
        "        dictionary = corpora.Dictionary(train_data_list_tokens)\n",
        "\n",
        "        # Compute coherence values and find optimal number of topics\n",
        "        coherence_values, optimal_num_topics = compute_coherence_values_lda(train_data_list_tokens, dictionary, max_topics)\n",
        "\n",
        "        # Plot coherence scores\n",
        "        plot_coherence_scores_lda(coherence_values, max_topics)\n",
        "\n",
        "        # Create Document-Term Matrix\n",
        "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in train_data_list_tokens]\n",
        "\n",
        "        # Train LDA model with optimal number of topics\n",
        "        lda_model = gensim.models.LdaModel(\n",
        "            doc_term_matrix,\n",
        "            num_topics=optimal_num_topics,\n",
        "            id2word=dictionary,\n",
        "            random_state=42,\n",
        "            passes=10,\n",
        "            alpha='auto'\n",
        "        )\n",
        "\n",
        "        # Get topics (top words in each topic)\n",
        "        topics = lda_model.print_topics(num_words=10)\n",
        "\n",
        "        return lda_model, topics\n",
        "\n",
        "    elif model_type == 'FLSA-W':\n",
        "        # Tokenized input is passed directly to FLSA-W\n",
        "        tokenized_data = [tokens for tokens in train_data['preprocessed_content']]\n",
        "\n",
        "        # Initialize FLSA-W model with tokenized data directly\n",
        "        flsa_w_model = FLSA_W(\n",
        "            input_file=tokenized_data,\n",
        "            num_topics=10,\n",
        "            num_words=10\n",
        "        )\n",
        "\n",
        "        # Train the FLSA-W model\n",
        "        pwgt, ptgd = flsa_w_model.get_matrices()  # This trains the model\n",
        "\n",
        "        # Get topics as words\n",
        "        topics = flsa_w_model.show_topics(representation='words')\n",
        "\n",
        "        return flsa_w_model, topics\n",
        "\n",
        "    elif model_type == 'BERTopic':\n",
        "        # Train BERTopic model\n",
        "        topic_model = BERTopic()\n",
        "        topics, _ = topic_model.fit_transform(train_data_list_string)\n",
        "\n",
        "        return topic_model, topics\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_type. Choose from 'LDA', 'FLSA-W', 'BERTopic'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b-vEbOayGaR"
      },
      "outputs": [],
      "source": [
        "def train_on_versions(data_versions, model_type, max_topics=20):\n",
        "    \"\"\"\n",
        "    Train a topic model on different versions of the data.\n",
        "\n",
        "    Parameters:\n",
        "    - data_versions: list of DataFrames, each containing a separate version of preprocessed data.\n",
        "    - model_type: str, the type of model to train ('LDA', 'FLSA-W', 'BERTopic')\n",
        "    - max_topics: int, the maximum number of topics to generate\n",
        "\n",
        "    Returns:\n",
        "    - models: list of trained models\n",
        "    - topics: list of topics generated by the models\n",
        "    \"\"\"\n",
        "\n",
        "    models = []\n",
        "    topics = []\n",
        "\n",
        "    # Train model on different versions\n",
        "    for data_v in data_versions:\n",
        "        print(f\"Training model on version: {data_v}\")\n",
        "        model, topic = train_topic_model(data_v, model_type=model_type, max_topics=max_topics)\n",
        "        print(f\"Finished training model on version: {data_v}\")\n",
        "        models.append(model)\n",
        "        topics.append(topic)\n",
        "\n",
        "    return models, topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de0Jh4K3J_o2"
      },
      "source": [
        "## Iteration 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZJ44yQKIYG"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "810-_HuwNVTw"
      },
      "outputs": [],
      "source": [
        "LDA_model, LDA_topics = train_topic_model(train_data_v1, model_type='LDA', max_topics=20)\n",
        "\n",
        "# Print topics\n",
        "for topic in LDA_topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm02RABvNXYu"
      },
      "source": [
        "### FLSA-W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz1RW5f9Ni9Q"
      },
      "outputs": [],
      "source": [
        "# Train FLSA-W model on the first version of the preprocessed dataset\n",
        "flsa_w_model, flsa_w_topics = train_topic_model(train_data_v1, model_type='FLSA-W', max_topics=10)\n",
        "\n",
        "# Print topics\n",
        "for topic in flsa_w_topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIkFdv0ZNli7"
      },
      "outputs": [],
      "source": [
        "# # Print the generated FLSA-W topics\n",
        "# print(\"Initial FLSA-W Topics:\")\n",
        "# print_topics_lda(flsa_w_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-nTTmxIN7jP"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_flsa_w(train_data, max_topics=10):\n",
        "    \"\"\"\n",
        "    Compute coherence scores for FLSA-W model with varying number of topics.\n",
        "    Parameters:\n",
        "        - train_data: list of tokenized documents (preprocessed content).\n",
        "        - max_topics: int, the maximum number of topics to test.\n",
        "    Returns:\n",
        "        - topic_nums: list of topic numbers used for testing.\n",
        "        - coherence_scores: list of coherence scores for each number of topics.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_data = [tokens for tokens in train_data['preprocessed_content']]\n",
        "\n",
        "    topic_nums = []\n",
        "    coherence_scores = []\n",
        "\n",
        "    # Try different numbers of topics\n",
        "    for num_topics in range(3, max_topics + 1):\n",
        "        # Train FLSA-W model\n",
        "        flsa_w_model = FLSA_W(\n",
        "            input_file=tokenized_data,\n",
        "            num_topics=num_topics,\n",
        "            num_words=10\n",
        "        )\n",
        "\n",
        "        # Train the FLSA-W model\n",
        "        flsa_w_model.get_matrices()\n",
        "\n",
        "        # Get the coherence score for the current model\n",
        "        coherence_score = flsa_w_model.get_coherence_score()\n",
        "        print(f\"Number of topics: {num_topics}, Coherence score: {coherence_score}\")\n",
        "\n",
        "        # Store the results\n",
        "        topic_nums.append(num_topics)\n",
        "        coherence_scores.append(coherence_score)\n",
        "\n",
        "    return topic_nums, coherence_scores\n",
        "\n",
        "\n",
        "def plot_elbow_curve(topic_nums, coherence_scores):\n",
        "    \"\"\"\n",
        "    Create an elbow plot for the coherence scores vs. number of topics.\n",
        "\n",
        "    Parameters:\n",
        "    - topic_nums: list of number of topics tested.\n",
        "    - coherence_scores: list of coherence scores for each number of topics.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(topic_nums, coherence_scores, marker='o')\n",
        "    plt.title('FLSA-W Elbow Plot: Number of Topics vs Coherence Score')\n",
        "    plt.xlabel('Number of Topics')\n",
        "    plt.ylabel('Coherence Score')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EhS5j6fOLQm"
      },
      "outputs": [],
      "source": [
        "# Train the model and compute coherence scores\n",
        "topic_nums, coherence_scores = compute_coherence_flsa_w(train_data_v1, max_topics=15)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plot_elbow_curve(topic_nums, coherence_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8JwRdpeONqM"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDhIsPONOXdS"
      },
      "outputs": [],
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v1, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuFOEwWUz1yn"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYY_NWeBOam7"
      },
      "source": [
        "## Iteration 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rk6CVxSz1yo"
      },
      "outputs": [],
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v2, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCrN0l0oOcXW"
      },
      "source": [
        "## Iteration 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hxrgkgcz5QB"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL6HDDnMz5QC"
      },
      "outputs": [],
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v3, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFUa24MPOeKP"
      },
      "source": [
        "## Iteration 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhkqta5Fz6mR"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTDDmxTVz6mR"
      },
      "outputs": [],
      "source": [
        "# Train BERTopic model on the first version of the preprocessed dataset\n",
        "bertopic_model, bertopic_topics = train_topic_model(train_data_v4, model_type='BERTopic')\n",
        "\n",
        "# Print the generated BERTopic topics\n",
        "print(\"BERTopic Topics:\")\n",
        "print_bertopic_topics(bertopic_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fArTnJ50J7O"
      },
      "source": [
        "Topic Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmkYFtmt0GXP"
      },
      "outputs": [],
      "source": [
        "bertopic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuoBS0hPOgJP"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9cBzPruOiXf"
      },
      "source": [
        "### Qualitative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkdD5fS0OmLW"
      },
      "source": [
        "### Quantitative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxD_9xRpOn4c"
      },
      "source": [
        "## Output of Final Topic Model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

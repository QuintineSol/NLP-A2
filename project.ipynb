{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from FuzzyTM import FLSA_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>release_date</th>\n",
       "      <th>provider</th>\n",
       "      <th>url</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221515</td>\n",
       "      <td>NIO</td>\n",
       "      <td>Why Shares of Chinese Electric Car Maker NIO A...</td>\n",
       "      <td>news</td>\n",
       "      <td>What s happening\\nShares of Chinese electric c...</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>https://invst.ly/pigqi</td>\n",
       "      <td>2060327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221516</td>\n",
       "      <td>NIO</td>\n",
       "      <td>NIO only consumer gainer  Workhorse Group amon...</td>\n",
       "      <td>news</td>\n",
       "      <td>Gainers  NIO  NYSE NIO   7  \\nLosers  MGP Ingr...</td>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>https://invst.ly/pje9c</td>\n",
       "      <td>2062196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221517</td>\n",
       "      <td>NIO</td>\n",
       "      <td>NIO leads consumer gainers  Beyond Meat and Ma...</td>\n",
       "      <td>news</td>\n",
       "      <td>Gainers  NIO  NYSE NIO   14   Village Farms In...</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>https://invst.ly/pifmv</td>\n",
       "      <td>2060249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221518</td>\n",
       "      <td>NIO</td>\n",
       "      <td>NIO  NVAX among premarket gainers</td>\n",
       "      <td>news</td>\n",
       "      <td>Cemtrex  NASDAQ CETX   85  after FY results \\n...</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>https://invst.ly/picu8</td>\n",
       "      <td>2060039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221519</td>\n",
       "      <td>NIO</td>\n",
       "      <td>PLUG  NIO among premarket gainers</td>\n",
       "      <td>news</td>\n",
       "      <td>aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>https://seekingalpha.com/news/3529772-plug-nio...</td>\n",
       "      <td>2053096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id ticker                                              title category  \\\n",
       "0  221515    NIO  Why Shares of Chinese Electric Car Maker NIO A...     news   \n",
       "1  221516    NIO  NIO only consumer gainer  Workhorse Group amon...     news   \n",
       "2  221517    NIO  NIO leads consumer gainers  Beyond Meat and Ma...     news   \n",
       "3  221518    NIO                  NIO  NVAX among premarket gainers     news   \n",
       "4  221519    NIO                  PLUG  NIO among premarket gainers     news   \n",
       "\n",
       "                                             content release_date  \\\n",
       "0  What s happening\\nShares of Chinese electric c...   2020-01-15   \n",
       "1  Gainers  NIO  NYSE NIO   7  \\nLosers  MGP Ingr...   2020-01-18   \n",
       "2  Gainers  NIO  NYSE NIO   14   Village Farms In...   2020-01-15   \n",
       "3  Cemtrex  NASDAQ CETX   85  after FY results \\n...   2020-01-15   \n",
       "4  aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...   2020-01-06   \n",
       "\n",
       "          provider                                                url  \\\n",
       "0  The Motley Fool                             https://invst.ly/pigqi   \n",
       "1    Seeking Alpha                             https://invst.ly/pje9c   \n",
       "2    Seeking Alpha                             https://invst.ly/pifmv   \n",
       "3    Seeking Alpha                             https://invst.ly/picu8   \n",
       "4    Seeking Alpha  https://seekingalpha.com/news/3529772-plug-nio...   \n",
       "\n",
       "   article_id  \n",
       "0     2060327  \n",
       "1     2062196  \n",
       "2     2060249  \n",
       "3     2060039  \n",
       "4     2053096  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./us_equities_news_dataset.csv\"\n",
    "\n",
    "# Load the news dataset\n",
    "news_dataset = pd.read_csv(\"./us_equities_news_dataset.csv\")\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to only include articles with 'Nvidia' in the content\n",
    "nvidia_dataset = news_dataset[news_dataset['content'].str.contains('Nvidia', case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text, \n",
    "                    remove_punctuation=True, \n",
    "                    remove_stopwords=True, \n",
    "                    lemmatize=False, \n",
    "                    stem=False, \n",
    "                    remove_short_words=False, \n",
    "                    remove_rare_words=False, \n",
    "                    remove_numbers=True, \n",
    "                    min_word_length=2):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing function that applies different levels of text processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The text to preprocess.\n",
    "    - remove_punctuation: Whether to remove punctuation from the text.\n",
    "    - remove_stopwords: Whether to remove common stopwords.\n",
    "    - lemmatize: Whether to apply lemmatization to reduce words to their root form.\n",
    "    - stem: Whether to apply stemming to reduce words to their base form.\n",
    "    - remove_short_words: Whether to remove short words from the text.\n",
    "    - remove_rare_words: Whether to remove rare words based on the dataset distribution.\n",
    "    - remove_numbers: Whether to remove numbers from the text.\n",
    "    - min_word_length: The minimum length of words to keep in the text.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed text as tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove URLs and HTML tags\n",
    "    tokens = [re.sub(r'http\\S+|www\\S+|<.*?>', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove numbers if specified\n",
    "    if remove_numbers:\n",
    "        tokens = [re.sub(r'\\d+', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove non-alphabetic characters (punctuation)\n",
    "    if remove_punctuation:\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Stemming (alternative to lemmatization)\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Remove short words\n",
    "    if remove_short_words:\n",
    "        tokens = [token for token in tokens if len(token) >= min_word_length]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(nvidia_dataset, version='v1'):\n",
    "    \"\"\"\n",
    "    Apply different levels of preprocessing to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - nvidia_dataset: The Nvidia articles with a 'content' column.\n",
    "    - version: The version of preprocessing to apply ('v1', 'v2', 'v3', or 'v4').\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the original content and the preprocessed content in 'preprocessed_content' column.\n",
    "    \"\"\"\n",
    "    \n",
    "    if version == 'v1':\n",
    "        # Basic tokenization and lowercasing\n",
    "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
    "            x, remove_punctuation=False, remove_stopwords=False, \n",
    "            lemmatize=False, remove_numbers=False, stem=False, \n",
    "            remove_short_words=False\n",
    "        ))\n",
    "    \n",
    "    elif version == 'v2':\n",
    "        # Remove punctuation, stopwords, and numbers, but no lemmatization/stemming\n",
    "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
    "            x, remove_punctuation=True, remove_stopwords=True, \n",
    "            lemmatize=False, remove_numbers=True, stem=False, \n",
    "            remove_short_words=False\n",
    "        ))\n",
    "    \n",
    "    elif version == 'v3':\n",
    "        # Advanced preprocessing with stemming, number removal, short words removal\n",
    "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
    "            x, remove_punctuation=True, remove_stopwords=True, \n",
    "            lemmatize=False, stem=True, remove_numbers=True, \n",
    "            remove_short_words=True, min_word_length=2\n",
    "        ))\n",
    "    \n",
    "    elif version == 'v4':\n",
    "        # Full preprocessing with lemmatization instead of stemming, number removal, and short words removal\n",
    "        nvidia_dataset['preprocessed_content'] = nvidia_dataset['content'].apply(lambda x: preprocess_text(\n",
    "            x, remove_punctuation=True, remove_stopwords=True, \n",
    "            lemmatize=True, remove_numbers=True, stem=False, \n",
    "            remove_short_words=True, min_word_length=2\n",
    "        ))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid preprocessing version. Choose from 'v1', 'v2', 'v3', or 'v4'.\")\n",
    "    \n",
    "    # Return the DataFrame with original and preprocessed content\n",
    "    return nvidia_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v1 = apply_preprocessing(nvidia_dataset, version='v1')  # Basic preprocessing\n",
    "train_data_v2 = apply_preprocessing(nvidia_dataset, version='v2')  # Intermediate preprocessing\n",
    "train_data_v3 = apply_preprocessing(nvidia_dataset, version='v3')  # Full preprocessing with stemming\n",
    "train_data_v4 = apply_preprocessing(nvidia_dataset, version='v4')  # Full preprocessing with lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>release_date</th>\n",
       "      <th>provider</th>\n",
       "      <th>url</th>\n",
       "      <th>article_id</th>\n",
       "      <th>preprocessed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>221539</td>\n",
       "      <td>NIO</td>\n",
       "      <td>A Central Bank War Just Started And Its Good F...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>ECB Effects\\nThe move in the euro was huge  fa...</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/a-central-b...</td>\n",
       "      <td>200395687</td>\n",
       "      <td>[ecb, effect, move, euro, huge, falling, pip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>221547</td>\n",
       "      <td>NIO</td>\n",
       "      <td>6 Stocks To Watch  Nivida Could Be Falling</td>\n",
       "      <td>opinion</td>\n",
       "      <td>6 Stocks To Watch  March 6 Trading Session\\nSt...</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/6-stocks-to...</td>\n",
       "      <td>200394931</td>\n",
       "      <td>[stock, watch, march, trading, session, stock,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>221572</td>\n",
       "      <td>NIO</td>\n",
       "      <td>Stocks   Dow Drops Nearly 400 Points as Apple ...</td>\n",
       "      <td>news</td>\n",
       "      <td>Investing com   A rout in Apple and Facebook  ...</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>1694042</td>\n",
       "      <td>[investing, com, rout, apple, facebook, nasdaq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>221593</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Zacks Analyst Blog Highlights  Advanced Mi...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>For Immediate ReleaseChicago  IL   January 13 ...</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-zacks-a...</td>\n",
       "      <td>200498277</td>\n",
       "      <td>[immediate, releasechicago, il, january, zacks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>221597</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Best Of CES 2020  Revised</td>\n",
       "      <td>opinion</td>\n",
       "      <td>With 4 500 companies bringing their innovation...</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-best-of...</td>\n",
       "      <td>200499164</td>\n",
       "      <td>[company, bringing, innovation, ce, jan, get, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id ticker                                              title category  \\\n",
       "24  221539    NIO  A Central Bank War Just Started And Its Good F...  opinion   \n",
       "32  221547    NIO         6 Stocks To Watch  Nivida Could Be Falling  opinion   \n",
       "57  221572    NIO  Stocks   Dow Drops Nearly 400 Points as Apple ...     news   \n",
       "78  221593   UBER  The Zacks Analyst Blog Highlights  Advanced Mi...  opinion   \n",
       "82  221597   UBER                     The Best Of CES 2020  Revised   opinion   \n",
       "\n",
       "                                              content release_date  \\\n",
       "24  ECB Effects\\nThe move in the euro was huge  fa...   2019-03-07   \n",
       "32  6 Stocks To Watch  March 6 Trading Session\\nSt...   2019-03-06   \n",
       "57  Investing com   A rout in Apple and Facebook  ...   2018-11-19   \n",
       "78  For Immediate ReleaseChicago  IL   January 13 ...   2020-01-12   \n",
       "82  With 4 500 companies bringing their innovation...   2020-01-16   \n",
       "\n",
       "                     provider  \\\n",
       "24             Michael Kramer   \n",
       "32             Michael Kramer   \n",
       "57              Investing.com   \n",
       "78  Zacks Investment Research   \n",
       "82  Zacks Investment Research   \n",
       "\n",
       "                                                  url  article_id  \\\n",
       "24  https://www.investing.com/analysis/a-central-b...   200395687   \n",
       "32  https://www.investing.com/analysis/6-stocks-to...   200394931   \n",
       "57  https://www.investing.com/news/stock-market-ne...     1694042   \n",
       "78  https://www.investing.com/analysis/the-zacks-a...   200498277   \n",
       "82  https://www.investing.com/analysis/the-best-of...   200499164   \n",
       "\n",
       "                                 preprocessed_content  \n",
       "24  [ecb, effect, move, euro, huge, falling, pip, ...  \n",
       "32  [stock, watch, march, trading, session, stock,...  \n",
       "57  [investing, com, rout, apple, facebook, nasdaq...  \n",
       "78  [immediate, releasechicago, il, january, zacks...  \n",
       "82  [company, bringing, innovation, ce, jan, get, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_head = train_data_v1.head()\n",
    "\n",
    "# # Specify the filename for the Excel file\n",
    "# output_file = 'train_data_v1_head.xlsx'\n",
    "\n",
    "# # Save to Excel\n",
    "# train_data_head.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_topic_model(train_data, model_type='LDA', num_topics=10):\n",
    "    \"\"\"\n",
    "    Train a topic model on the given training data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: list of str, the text to train the model on\n",
    "    - model_type: str, the type of model to train ('LDA', 'FLSA-W', 'BERTopic')\n",
    "    - num_topics: int, the number of topics to generate\n",
    "\n",
    "    Returns:\n",
    "    - model: the trained model\n",
    "    - topics: the topics generated by the model\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_list = [' '.join(tokens) for tokens in train_data['preprocessed_content']]\n",
    "\n",
    "    if model_type == 'LDA':\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "        data_vectorized = vectorizer.fit_transform(train_data_list)\n",
    "        \n",
    "        # Train LDA model\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "        lda_model.fit(data_vectorized)\n",
    "        \n",
    "        # Get topics (top words in each topic)\n",
    "        def get_lda_topics(model, vectorizer, n_top_words=10):\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            topics = []\n",
    "            for topic_idx, topic in enumerate(model.components_):\n",
    "                top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "                topics.append(top_words)\n",
    "            return topics\n",
    "        \n",
    "        topics = get_lda_topics(lda_model, vectorizer)\n",
    "        \n",
    "        return lda_model, topics\n",
    "    \n",
    "    elif model_type == 'FLSA-W':\n",
    "        # Replace with actual FLSA-W training code\n",
    "        flsa_w_model = None\n",
    "        topics = None\n",
    "        \n",
    "        return flsa_w_model, topics\n",
    "    \n",
    "    elif model_type == 'BERTopic':\n",
    "        # Train BERTopic model\n",
    "        topic_model = BERTopic(nr_topics=num_topics)\n",
    "        topics, _ = topic_model.fit_transform(train_data_list)\n",
    "        \n",
    "        return topic_model, topics\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'LDA', 'FLSA-W', 'BERTopic'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_lda(topics):\n",
    "    for idx, topic in enumerate(topics):\n",
    "        print(f\"Topic {idx + 1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_flsaw(topics):\n",
    "    for idx, topic in enumerate(topics):\n",
    "        print(f\"Topic {idx + 1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bertopic_topics(topic_model):\n",
    "    \"\"\"\n",
    "    Print topics generated by BERTopic.\n",
    "    \n",
    "    Parameters:\n",
    "    - topic_model: the trained BERTopic model\n",
    "    \"\"\"\n",
    "    topics = topic_model.get_topics()\n",
    "    for topic_num, words in topics.items():\n",
    "        # Ignore the '-1' topic, which is typically noise in BERTopic\n",
    "        if topic_num == -1:\n",
    "            continue\n",
    "        print(f\"Topic {topic_num}: {', '.join([word[0] for word in words])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA  model on the first version of the preprocessed dataset\n",
    "LDA_model, LDA_topics = train_topic_model(train_data_v1, model_type='LDA', num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial LDA Topics:\n",
      "Topic 1: company, apple, nasdaq, technology, google, market, new, amazon, year, service\n",
      "Topic 2: market, week, year, earnings, index, rate, investor, stock, price, month\n",
      "Topic 3: year, quarter, company, zacks, revenue, earnings, million, estimate, stock, growth\n",
      "Topic 4: driving, car, company, vehicle, self, technology, autonomous, said, bitcoin, nasdaq\n",
      "Topic 5: etf, fund, semiconductor, zacks, technology, nasdaq, index, read, portfolio, company\n",
      "Topic 6: stock, zacks, earnings, year, investment, company, growth, industry, market, investor\n",
      "Topic 7: nasdaq, nyse, stock, trade, share, rose, percent, fell, point, china\n",
      "Topic 8: nasdaq, stock, day, nyse, trading, nvda, week, market, support, short\n",
      "Topic 9: stock, market, year, like, investor, time, new, week, share, think\n",
      "Topic 10: amd, intel, gaming, chip, year, market, analyst, company, revenue, nasdaq\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial LDA Topics:\")\n",
    "print_topics_lda(LDA_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLSA-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Train FLSA-W model on the first version of the preprocessed dataset\n",
    "\n",
    "# Print the generated FLSA-W topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BERTopic model on the first version of the preprocessed dataset\n",
    "# num_topics is maximum number of topics rather than a fixed number of topics\n",
    "bertopic_model, bertopic_topics = train_topic_model(train_data_v1, model_type='BERTopic', num_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Topics:\n",
      "Topic 0: year, company, zacks, quarter, stock, revenue, earnings, nasdaq, million, growth\n",
      "Topic 1: nasdaq, stock, nyse, week, market, trade, index, day, inc, year\n",
      "Topic 2: nvidia, year, gaming, revenue, company, quarter, share, nvda, billion, nasdaq\n",
      "Topic 3: driving, car, vehicle, self, autonomous, technology, company, ai, system, nvidia\n",
      "Topic 4: traded, nvidia, day, seven, lowest, gmt, highest, exchange, volume, session\n",
      "Topic 5: earnings, growth, estimate, zacks, stock, eps, period, company, investment, quarter\n",
      "Topic 6: huawei, said, qualcomm, china, company, chinese, technology, patent, commission, chip\n",
      "Topic 7: facebook, user, ad, company, platform, video, zacks, nasdaq, rank, twitter\n",
      "Topic 8: rallied, climbed, soared, australian, gained, italy, market, dollar, step, towards\n"
     ]
    }
   ],
   "source": [
    "# Print the generated BERTopic topics\n",
    "print(\"BERTopic Topics:\")\n",
    "print_bertopic_topics(bertopic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of Final Topic Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JADS-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
